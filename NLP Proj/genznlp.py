# -*- coding: utf-8 -*-
"""GenzNLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13ut58B4xn36rzjvPK_RisRfc0SfuvyqW
"""

# Try flan-t5-small (better instruction-following ability)
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")

input_text = "Rephrase the following into formal English: bruh she was lowkey ghosting me but I kept it chill fr"
inputs = tokenizer(input_text, return_tensors="pt")

outputs = model.generate(inputs["input_ids"], max_length=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

from transformers import T5Tokenizer, T5ForConditionalGeneration

# Load pre-trained T5 model and tokenizer (base version is good enough to start)
model = T5ForConditionalGeneration.from_pretrained('t5-base')
tokenizer = T5Tokenizer.from_pretrained('t5-base')

# Input: Gen Z slang sentence
genz_text = "translate genz to proper english: bruh she was lowkey ghosting me but I kept it chill fr"

# Tokenize input
inputs = tokenizer(genz_text, return_tensors="pt", padding=True)

# Generate output
outputs = model.generate(inputs["input_ids"], max_length=50, num_beams=4, early_stopping=True)

# Decode and print
translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(translated_text)

"""## Cleaning Script
---
"""

import pandas as pd

# Input and output paths
input_path = "/content/drive/MyDrive/Colab Notebooks/NLP/genz_finetune_dataset.csv"
output_path = "/content/drive/MyDrive/Colab Notebooks/NLP/genz_finetune_dataset(1).csv"

# Load CSV
df = pd.read_csv(input_path)

# Strip whitespace from column names
df.columns = df.columns.str.strip()

# Display original row count
print(f"Original number of rows: {len(df)}")

# Drop rows where 'Example' or 'Proper English' or 'Description' is missing or empty
df_cleaned = df.dropna(subset=["input", "target"])
df_cleaned = df_cleaned[
    (df_cleaned["input"].str.strip() != "") &
    (df_cleaned["target"].str.strip() != "")
]

# Display cleaned row count
print(f"Cleaned number of rows: {len(df_cleaned)}")

# Save cleaned dataset
df_cleaned.to_csv(output_path, index=False)
print(f"Cleaned dataset saved to: {output_path}")

"""## Zero-Shot Testing
---
"""

# üìò Zero-Shot Evaluation of Gen-Z to Formal English Translation

## üß∞ Install Required Libraries
!pip install transformers datasets sentence-transformers evaluate --quiet

## üîÑ Load Required Libraries
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sentence_transformers import SentenceTransformer, util
from datasets import Dataset
import pandas as pd
import torch
from tqdm import tqdm

## üìÇ Load Your Dataset
# Assuming your dataset is a CSV file with columns: 'Slang', 'Description', 'Example', 'Proper English'
df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/NLP/genZ_slangs(1.7k_rows)_edited.csv")

## üß™ Define the Prompt Template
def build_prompt(example, use_description=True):
    if use_description:
        return f"Paraphrase the following Gen-Z sentence into formal English:\n\nDescription: {example['Description']}\nExample: {example['Example']}"
    else:
        return f"Paraphrase into formal English: {example['Example']}"

print(df.columns.tolist())

## üìê Define Evaluation Function using Sentence-BERT
embedder = SentenceTransformer('all-MiniLM-L6-v2')

def semantic_similarity(preds, refs):
    pred_emb = embedder.encode(preds, convert_to_tensor=True)
    ref_emb = embedder.encode(refs, convert_to_tensor=True)
    return util.cos_sim(pred_emb, ref_emb).diagonal().tolist()

## üß™ Zero-Shot Inference Function
def evaluate_model(model_name, df, limit=100):
    print(f"\nEvaluating: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    model.eval()

    preds, refs, sims = [], [], []
    for i, row in tqdm(df.head(limit).iterrows(), total=limit):
        prompt = build_prompt(row)
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True)
        with torch.no_grad():
            output = model.generate(**inputs, max_length=60)
        pred = tokenizer.decode(output[0], skip_special_tokens=True)
        preds.append(pred)
        refs.append(row["Proper English"])

    sims = semantic_similarity(preds, refs)
    avg_sim = sum(sims) / len(sims)
    print(f"Average Semantic Similarity: {avg_sim:.4f}")
    return {
        "model": model_name,
        "avg_similarity": avg_sim,
        "predictions": preds,
        "references": refs,
        "similarities": sims
    }

## üß™ Evaluate Multiple Models
models_to_test = [
    "google/flan-t5-base",
    "google/flan-t5-large",
    "google/t5-v1_1-base",
    "facebook/bart-large"
]

results = []
for model_name in models_to_test:
    result = evaluate_model(model_name, df)
    results.append(result)

## üñ®Ô∏è Print Summary Table
summary = pd.DataFrame([{"Model": r["model"], "Avg Semantic Similarity": r["avg_similarity"]} for r in results])
print("\nüìä Summary of Results:")
print(summary)

"""CSV conversion into fine-tuning format."""

import pandas as pd

# Load dataset
df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/NLP/genZ_slangs(1.7k_rows)_edited.csv")

# Clean column names
df.columns = df.columns.str.strip()

# Create prompt-target format
formatted_data = []
for _, row in df.iterrows():
    input_text = f"Paraphrase this Gen-Z sentence into formal English:\nExample: {row['Example']}\nDescription: {row['Description']}"
    target_text = row["Proper English"]
    formatted_data.append({"input": input_text, "target": target_text})

# Convert to dataframe and save
formatted_df = pd.DataFrame(formatted_data)
formatted_df.to_csv("/content/drive/MyDrive/Colab Notebooks/NLP/genz_finetune_dataset.csv", index=False)

"""Finetuning"""

# üìò Fine-Tuning FLAN-T5 on Gen-Z to Formal English Translation

## üß∞ Install Required Libraries
!pip install transformers datasets evaluate accelerate rouge_score bert_score --quiet

## üîÑ Import Libraries
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq
from datasets import load_dataset, Dataset
import pandas as pd
import torch
import evaluate
import numpy as np

## üìÇ Load and Prepare Dataset
df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/NLP/genz_finetune_dataset(1).csv")
# Strip and clean
df.columns = df.columns.str.strip()
df = df.dropna(subset=["input", "target"])
df = df[df["input"].str.strip() != ""]
df = df[df["target"].str.strip() != ""]

# Keep only relevant columns
df = df[["input", "target"]]  # <- THIS is important!

# Convert safely
dataset = Dataset.from_pandas(df)

# Train/Validation split
dataset = dataset.train_test_split(test_size=0.1)
train_dataset = dataset['train']
val_dataset = dataset['test']

for row in tokenized_train.select(range(5)):
    print("Input IDs:", row['input_ids'])
    print("Labels:", row['labels'])

"""api key: c9fb887ca673b937660a448fdc45d257f1c32d77"""

## Tokenization
model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def preprocess(example):
    input_text = str(example['input']).strip()
    target_text = str(example['target']).strip()

    if input_text == "" or target_text == "":
        return {}

    model_inputs = tokenizer(input_text, truncation=True, padding="max_length", max_length=128)
    labels = tokenizer(target_text, truncation=True, padding="max_length", max_length=128)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_train = train_dataset.map(preprocess, batched=False).filter(lambda x: "labels" in x)
tokenized_val = val_dataset.map(preprocess, batched=False).filter(lambda x: "labels" in x)

## Load Model
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

## Metric Functions
meteor = evaluate.load("meteor")
bertscore = evaluate.load("bertscore")

def compute_metrics(eval_preds):
    preds, labels = eval_preds

    # Replace -100s in labels for proper decoding
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

    # Ensure preds are a NumPy array of int32
    preds = np.array(preds).astype(np.int32)

    # Hard clip values to valid token range
    preds = np.clip(preds, 0, tokenizer.vocab_size - 1)
    labels = np.clip(labels, 0, tokenizer.vocab_size - 1)

    # Defensive decode with try/except to avoid corrupt entries
    try:
        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    except OverflowError as e:
        print("‚ö†Ô∏è Skipping decoding due to invalid token ids")
        return {
            "meteor": 0.0,
            "bertscore_f1": 0.0
        }

    # Strip and filter empty ones
    decoded_preds = [p.strip() for p in decoded_preds]
    decoded_labels = [l.strip() for l in decoded_labels]

    paired = [(p, l) for p, l in zip(decoded_preds, decoded_labels) if p and l]
    if not paired:
        return {
            "meteor": 0.0,
            "bertscore_f1": 0.0
        }

    filtered_preds, filtered_labels = zip(*paired)

    result = {}
    result.update(meteor.compute(predictions=filtered_preds, references=filtered_labels))
    result["bertscore_f1"] = np.mean(
        bertscore.compute(predictions=filtered_preds, references=filtered_labels, lang="en")["f1"]
    )

    return {
        "meteor": result["meteor"],
        "bertscore_f1": result["bertscore_f1"],
    }

## Training Arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./flan-t5-genz",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=5,
    weight_decay=0.01,
    save_total_limit=1,
    predict_with_generate=True,
    logging_dir='./logs',
    logging_strategy="steps",
    logging_steps=10,
    save_strategy="epoch",
    load_best_model_at_end=True
)

## Data Collator
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

## Initialize Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

## Model Trainer
trainer.train()

## üíæ Save Final Model
model.save_pretrained("/content/drive/MyDrive/Colab Notebooks/NLP/fine-tuned models/flan-t5-genz-final")
tokenizer.save_pretrained("/content/drive/MyDrive/Colab Notebooks/NLP/fine-tuned models/flan-t5-genz-final")

"""## Inference Testing
---
"""

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import pandas as pd

# === ‚úÖ Load Fine-Tuned Model ===
model_path = "/content/drive/MyDrive/Colab Notebooks/NLP/fine-tuned models/flan-t5-genz-final"  # adjust this path if needed
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSeq2SeqLM.from_pretrained(model_path)

# Use GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
model.eval()

# === üìù Load Input Prompts for Testing ===
df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/NLP/genz_finetune_dataset.csv")  # use any CSV with 'input' column
test_inputs = df['input'].dropna().sample(5).tolist()

# === üöÄ Run Inference ===
for inp in test_inputs:
    encoded = tokenizer(inp, return_tensors="pt", truncation=True, padding=True).to(device)
    output = model.generate(**encoded, max_length=60)
    decoded = tokenizer.decode(output[0], skip_special_tokens=True)

    print(f"üß† Prompt: {inp}")
    print(f"‚úÖ Output: {decoded}\\n")

"""## User inupt test
---
"""

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import pandas as pd

# === ‚úÖ Load Fine-Tuned Model ===
model_path = "/content/drive/MyDrive/Colab Notebooks/NLP/fine-tuned models/flan-t5-genz-final"  # path to your fine-tuned model
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSeq2SeqLM.from_pretrained(model_path)

# Use GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
model.eval()

# === üß™ Interactive User Input ===
print("=== üí¨ Try It Yourself ===")
while True:
    user_input = input("Enter a Gen-Z sentence (or type 'exit'): ").strip()
    if user_input.lower() in ["exit", "quit"]:
        break

    # Construct the prompt using same format as training
    prompt = f"Paraphrase this Gen-Z sentence into formal English:\nExample: {user_input}"

    encoded = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True).to(device)
    output = model.generate(**encoded, max_length=60)
    decoded = tokenizer.decode(output[0], skip_special_tokens=True)

    print(f"üìù Formal English: {decoded}\n")